{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c644bb48-2efe-4dff-8854-19484fdfe933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "N of woodlawn.csv  476\n",
      "woodlawn.csv True\n",
      "------------------------------\n",
      "N of hyde_park.csv  497\n",
      "hyde_park.csv True\n",
      "------------------------------\n",
      "N of outcome_data.csv  989\n",
      "outcome_data.csv False\n",
      "------------------------------\n",
      "N of treatment.csv  1004\n",
      "treatment.csv False\n"
     ]
    }
   ],
   "source": [
    "# ===Check the uniqueness of subject_is in each file===\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# List of file names in problem set\n",
    "files = ['woodlawn.csv', 'hyde_park.csv', 'outcome_data.csv', 'treatment.csv']\n",
    "\n",
    "# using for loop, read the file one by one.\n",
    "for file in files:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file)\n",
    "        \n",
    "    # use .is_unique to check values of subject_id are unique, and show the result with the numebr of observation in each file.\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"N of {file} \",len(df),)\n",
    "    print(file, df['subject_id'].is_unique)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fd2d122-a06a-4cc1-b5fb-170a74e1bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Duplicates subject_id of outcome_data.csv  ---\n",
      "     subject_id  hours\n",
      "243         NaN    NaN\n",
      "779         NaN    NaN\n",
      "--- Duplicates subject_id of treatment.csv  ---\n",
      "      subject_id  treatment\n",
      "32            33          0\n",
      "1000          33          0\n",
      "33            34          1\n",
      "1001          34          1\n",
      "57            58          0\n",
      "1002          58          0\n",
      "1003          58          0\n"
     ]
    }
   ],
   "source": [
    "# ===Check obsevations with non-unique subject id ====\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# List of file names which cantain dupulicate ids\n",
    "files_to_check = ['outcome_data.csv', 'treatment.csv']\n",
    "\n",
    "for file in files_to_check:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    #  Extract all rows with duplicate subject_ids\n",
    "    # Setting keep=False displays all occurrences, including the first one\n",
    "    duplicates = df[df.duplicated(subset=['subject_id'], keep=False)]\n",
    "    \n",
    "    # Sorting by ID \n",
    "    duplicates = duplicates.sort_values(by='subject_id')\n",
    "    \n",
    "    print(f\"--- Duplicates subject_id of {file}  ---\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f055a77-084c-4bb9-8dae-13217e9dc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values in woodlawn.csv ---\n",
      "race_ethnicity    10\n",
      "dtype: int64\n",
      "--- Missing Values in hyde_park.csv ---\n",
      "race_ethnicity    17\n",
      "dtype: int64\n",
      "--- Missing Values in outcome_data.csv ---\n",
      "subject_id    2\n",
      "hours         2\n",
      "dtype: int64\n",
      "--- Missing Values in treatment.csv ---\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# ====Check whether each file includes missing values====\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# List of target files\n",
    "files = ['woodlawn.csv', 'hyde_park.csv', 'outcome_data.csv', 'treatment.csv']\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    print(f\"--- Missing Values in {file} ---\")\n",
    "    \n",
    "    # Identify missing values with isnull() and calculate the sum per column\n",
    "    missing_counts = df.isnull().sum()\n",
    "    \n",
    "    # Filter and display only columns that have at least one missing value for better readability\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23489eb7-c5d5-438b-9769-1d5e1d18d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name            | Before     | After      | Removed\n",
      "-------------------------------------------------------\n",
      "outcome_data.csv     | 989        | 987        | 2\n",
      "treatment.csv        | 1004       | 997        | 7\n"
     ]
    }
   ],
   "source": [
    "# ====Cleaning dataset_1:drop obsevations with non-unique id====\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# List of datasets which has non unique id\n",
    "files = ['outcome_data.csv', 'treatment.csv']\n",
    "\n",
    "print(f\"{'File Name':<20} | {'Before':<10} | {'After':<10} | {'Removed'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Using for loop, drop obsevations with non-unique subject id in each file. \n",
    "for file in files:\n",
    "    # Load the original data\n",
    "    df = pd.read_csv(file)\n",
    "    initial_n = len(df)\n",
    "    \n",
    "    # Cleaning Step 1: Remove missing subject_ids\n",
    "    df_no_na = df.dropna(subset=['subject_id'])\n",
    "    \n",
    "    # Cleaning Step 2: Remove duplicate subject_ids\n",
    "    df_final = df_no_na.drop_duplicates(subset=['subject_id'], keep=False)\n",
    "    final_n = len(df_final)\n",
    "    \n",
    "    # Calculate how many rows were deleted\n",
    "    removed = initial_n - final_n\n",
    "    \n",
    "    # Print the results in a table format\n",
    "    print(f\"{file:<20} | {initial_n:<10} | {final_n:<10} | {removed}\")\n",
    "\n",
    "    df_final.to_csv(f\"removed_non_unique_ID_{file}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e61c89b-05cd-4ca7-94d7-c7514a1af7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 973\n",
      "Check Unique: True\n"
     ]
    }
   ],
   "source": [
    "# ===intergarte \"woodlawn\" and \"hyde_parkand\" using subject_id as a key.===\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the community datasets\n",
    "df_woodlawn = pd.read_csv('woodlawn.csv')\n",
    "df_hyde_park = pd.read_csv('hyde_park.csv')\n",
    "\n",
    "# 2. Load the pre-cleaned treatment and outcome datasets\n",
    "df_treatment_cleaned = pd.read_csv('removed_non_unique_ID_treatment.csv')\n",
    "df_outcome_cleaned = pd.read_csv('removed_non_unique_ID_outcome_data.csv')\n",
    "\n",
    "# 3. Vertically stack the community center datasets (The \"Master List\")\n",
    "df_community_combined = pd.concat([df_woodlawn, df_hyde_park])\n",
    "\n",
    "# 4. Perform LEFT JOIN with treatment data\n",
    "# 'how=left' ensures all observations in dataset of two comunnity centers are kept, even if missing in treatment\n",
    "df_merged_step1 = pd.merge(df_community_combined, df_treatment_cleaned, on='subject_id', how='left')\n",
    "\n",
    "# 5. Perform LEFT JOIN with outcome data\n",
    "# This keeps the observation even if they didn't record any 'hours'\n",
    "df_final_left_merged = pd.merge(df_merged_step1, df_outcome_cleaned, on='subject_id', how='left')\n",
    "\n",
    "# 6. Export the final data (using utf-8-sig for Excel compatibility)\n",
    "df_final_left_merged.to_csv('merged_left_data_removed_nonunique_id_not_clearned_forQ1Q2.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 7. Report the numbers\n",
    "print(f\"Total Rows: {len(df_final_left_merged)}\")\n",
    "print(\"Check Unique:\",df_final_left_merged['subject_id'].is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6b4666f-dbf0-467e-8a1f-414ba8289be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start N: 973\n",
      "1. After removing missing values: 930\n",
      "2. After removing negative age: 876\n",
      "3. After removing hours > 100: 840\n",
      "4. After removing BMI < 1.0: 815\n",
      "5. After removing invalid treatment codes: 815\n",
      "\n",
      "Cleaning process complete.\n",
      "Final N: 815\n",
      "File saved as: cleaned_data_forQ3_Q9.csv\n"
     ]
    }
   ],
   "source": [
    "# ===Cleaning dataset_2: drop the observation with some missing values or invalid values===\n",
    "\n",
    "# --prompt--\n",
    "# 以下の手順で、Pythonコードを書いて。\n",
    "# merged_left_data_removed_nonunique_id_not_clearned_forQ1Q2.csv を読み込んで。最初に Start N: [行数] を表示して。\n",
    "# フィルタリング（各ステップで行数を表示して）\n",
    "# 欠損値（NaN）がある行をすべて削除して。\n",
    "# age がマイナスの行を削除して。\n",
    "# hours が100を超える行を削除して。\n",
    "# bmi が1.0未満の行を削除して。\n",
    "# treatment が 0 と 1 以外の行を削除して。\n",
    "# community_center 列を小文字にして空白を除去して。'woodlawn' は 0 に、'hyde park', 'hydepark', 'hyde_park', 'hyd park' はすべて 1 に変換して。\n",
    "# female 列の \"female\" を 1、\"male\" を 0 に変換して。最終的に数値型（int）になるようにして。\n",
    "# education と race_ethnicity はそれぞれ小文字にして空白を除去して。\n",
    "# pd.get_dummies を使ってダミー変数を作って。結果が True/False ではなく 1 と 0（int型） になるようにして。\n",
    "# 教育歴は \"less than high school\", \"high school\", \"higher degree\" の3列、人種は \"white\", \"black\", \"hispanic\" の3列を元のデータに結合して。\n",
    "# 最後に subject_id の昇順でデータを並べ替えて。\n",
    "#ファイル名を cleaned_data_forQ3_Q9.csv にして保存して。Excelで開いても列が分かれるように encoding='utf-8-sig' を使って、インデックスは含めないで。\n",
    "# 最後に Final N: [行数] を表示して。\n",
    "# ----\n",
    "\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Opt-in to future Pandas behavior to suppress warnings when converting data types during replace operations\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Load the dataset\n",
    "# Note: Ensure the file name matches exactly as provided\n",
    "df = pd.read_csv('merged_left_data_removed_nonunique_id_not_clearned_forQ1Q2.csv')\n",
    "\n",
    "# Initial observation count\n",
    "print(f\"Start N: {len(df)}\")\n",
    "\n",
    "# 1. Remove observations with missing values in any columns\n",
    "df = df.dropna()\n",
    "print(f\"1. After removing missing values: {len(df)}\")\n",
    "\n",
    "# 2. Remove observations with negative age\n",
    "df = df[df['age'] >= 0]\n",
    "print(f\"2. After removing negative age: {len(df)}\")\n",
    "\n",
    "# 3. Remove observations where hours are more than 100\n",
    "df = df[df['hours'] <= 100]\n",
    "print(f\"3. After removing hours > 100: {len(df)}\")\n",
    "\n",
    "# 4. Remove observations where bmi is less than 1.0\n",
    "df = df[df['bmi'] >= 1.0]\n",
    "print(f\"4. After removing BMI < 1.0: {len(df)}\")\n",
    "\n",
    "# 5. Remove observations where treatment is not 0 or 1\n",
    "df = df[df['treatment'].isin([0, 1])]\n",
    "print(f\"5. After removing invalid treatment codes: {len(df)}\")\n",
    "\n",
    "# 6 & 7. Standardize Community Center names (Woodlawn -> 0, Hyde Park -> 1)\n",
    "# Use .str.lower() for case-insensitivity\n",
    "df['community_center'] = df['community_center'].astype(str).str.lower().str.strip()\n",
    "df['community_center'] = df['community_center'].replace({\n",
    "    'woodlawn': 0, \n",
    "    'hyde park': 1, \n",
    "    'hydepark': 1,\n",
    "    'hyde_park':1,\n",
    "    'hyd park':1,\n",
    "})\n",
    "\n",
    "# 8. Convert \"female\" to 1 and \"male\" to 0\n",
    "df['female'] = df['female'].astype(str).str.lower().str.strip()\n",
    "df['female'] = df['female'].replace({'female': 1, 'male': 0})\n",
    "# Ensure the column is numeric integer\n",
    "df['female'] = pd.to_numeric(df['female']).astype(int)\n",
    "\n",
    "# 9 & 10. Process Education: Lowercase and convert to 1/0 dummy variables\n",
    "df['education'] = df['education'].str.lower().str.strip()\n",
    "# dtype=int ensures the dummies are 1/0 instead of True/False\n",
    "edu_dummies = pd.get_dummies(df['education'], dtype=int)\n",
    "edu_cols = [\"less than high school\", \"high school\", \"higher degree\"]\n",
    "# Concatenate specific dummy columns to the main dataframe\n",
    "df = pd.concat([df, edu_dummies[edu_cols]], axis=1)\n",
    "\n",
    "# 11 & 12. Process Race/Ethnicity: Lowercase and convert to 1/0 dummy variables\n",
    "df['race_ethnicity'] = df['race_ethnicity'].str.lower().str.strip()\n",
    "race_dummies = pd.get_dummies(df['race_ethnicity'], dtype=int)\n",
    "race_cols = [\"white\", \"black\", \"hispanic\"]\n",
    "# Concatenate specific dummy columns to the main dataframe\n",
    "df = pd.concat([df, race_dummies[race_cols]], axis=1)\n",
    "\n",
    "# Reorder rows based on the subject_id column in ascending order (from 1, 2, 3...)\n",
    "df = df.sort_values(by='subject_id')\n",
    "\n",
    "# Final step: Save the cleaned data to a new CSV file\n",
    "output_filename = 'cleaned_data_forQ3_Q9.csv'\n",
    "df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nCleaning process complete.\")\n",
    "print(f\"Final N: {len(df)}\")\n",
    "print(f\"File saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00824134-95ec-44de-98f1-e1ccc87cc41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Integrity Report ---\n",
      "IDs with missing values in raw files: [np.int64(65), np.int64(66), np.int64(104), np.int64(134), np.int64(149), np.int64(168), np.int64(183), np.int64(202), np.int64(233), np.int64(236), np.int64(272), np.int64(276), np.int64(305), np.int64(312), np.int64(313), np.int64(326), np.int64(355), np.int64(476), np.int64(487), np.int64(558), np.int64(576), np.int64(597), np.int64(630), np.int64(708), np.int64(820), np.int64(890), np.int64(941)]\n",
      "Total count of IDs with missing values: 27\n",
      "Note: 2 records had missing subject_ids entirely (untrackable).\n",
      "\n",
      "Total IDs dropped from the 1-1000 range: 185\n",
      "Dropped IDs: [16, 17, 26, 33, 34, 38, 42, 58, 59, 62, 65, 66, 67, 83, 86, 101, 103, 104, 105, 114, 126, 129, 134, 139, 145, 149, 154, 155, 157, 164, 167, 168, 183, 184, 185, 189, 190, 201, 202, 209, 211, 214, 233, 236, 237, 241, 246, 248, 261, 262, 269, 272, 276, 287, 288, 296, 298, 299, 305, 312, 313, 317, 326, 337, 343, 352, 353, 355, 362, 371, 381, 397, 401, 402, 403, 415, 431, 432, 445, 446, 449, 457, 464, 466, 474, 475, 476, 480, 483, 485, 487, 495, 500, 503, 504, 507, 509, 513, 527, 539, 545, 547, 558, 568, 576, 586, 587, 595, 597, 598, 613, 621, 622, 623, 629, 630, 636, 653, 655, 660, 669, 672, 676, 680, 682, 683, 686, 706, 708, 710, 721, 723, 735, 770, 779, 785, 791, 807, 809, 810, 813, 820, 822, 825, 830, 845, 847, 856, 863, 866, 869, 871, 874, 875, 889, 890, 891, 892, 901, 904, 906, 908, 917, 924, 925, 928, 929, 932, 941, 943, 946, 952, 958, 961, 965, 967, 973, 975, 979, 984, 985, 991, 994, 997, 998]\n",
      "\n",
      "--- Verification ---\n",
      "✅ Success: All IDs identified with missing values were removed from the final dataset.\n"
     ]
    }
   ],
   "source": [
    "# ====Check the cleaned data====\n",
    "# Verify that IDs with missing values in the original datasets are correctly excluded from the final cleaned data\n",
    "\n",
    "# --prompt--\n",
    "# 以下の4つのファイルをリストとしてループ処理。\n",
    "# woodlawn.csv, hyde_park.csv, outcome_data.csv, treatment.csv\n",
    "# 各ファイル内で、いずれかのカラム（列）に1つでも欠損値（NaN）がある行の subject_id をすべて抽出し、重複のないセットにまとめて。\n",
    "# ただし、subject_id 自体が欠損している行はIDで追跡できないため、別途 outcome_data.csv において subject_id が欠損している行数のみをカウントして。\n",
    "# cleaned_data_forQ3_Q9.csv を読み込み、そこに含まれる実際のIDを取得。\n",
    "# 1から1000までのID範囲を定義してください。\n",
    "# この1〜1000の範囲の中で、最終データに残っていないIDを「ドロップされたID」として特定\n",
    "# 「1つのカラムでも欠損値があったID」が、すべて「最終データからドロップされたID」に含まれているかを確認して。\n",
    "# 以下の情報を表示\n",
    "# 元のファイルで欠損値があったIDのリストと合計数\n",
    "# ID自体が欠損していた件数（追跡不能分）\n",
    "# 1〜1000の範囲から実際にドロップされたIDの数とリスト\n",
    "# 検証結果「すべての欠損値IDが正しく削除された」、またはまだデータに残っている問題のあるIDのリストを表示\n",
    "# ----\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the raw datasets to find IDs with missing values\n",
    "raw_files = ['woodlawn.csv', 'hyde_park.csv', 'outcome_data.csv', 'treatment.csv']\n",
    "ids_with_missing_values = set()\n",
    "\n",
    "for file in raw_files:\n",
    "    df_raw = pd.read_csv(file)\n",
    "    # Identify rows where any column has a NaN value\n",
    "    # We extract the 'subject_id' of these rows\n",
    "    # .dropna(subset=['subject_id']) ensures we only collect IDs that actually exist\n",
    "    m_ids = df_raw[df_raw.isnull().any(axis=1)]['subject_id'].dropna().unique()\n",
    "    ids_with_missing_values.update(m_ids)\n",
    "\n",
    "# 2. Identify IDs in outcome_data where subject_id itself was NaN (if any)\n",
    "# These cannot be tracked by ID, so we just count them\n",
    "df_outcome = pd.read_csv('outcome_data.csv')\n",
    "nan_id_count = df_outcome['subject_id'].isnull().sum()\n",
    "\n",
    "# 3. Load the final cleaned dataset (which should be a subset of 1-1000)\n",
    "df_cleaned = pd.read_csv('cleaned_data_forQ3_Q9.csv')\n",
    "actual_ids = set(df_cleaned['subject_id'].unique())\n",
    "\n",
    "# 4. Identify which IDs from the 1-1000 range are missing in the final data\n",
    "expected_range = set(range(1, 1001))\n",
    "dropped_ids = expected_range - actual_ids\n",
    "\n",
    "# 5. Cross-check: Are the \"missing value IDs\" included in the \"dropped IDs\"?\n",
    "# This tells us if the people with missing data were successfully removed\n",
    "confirmed_drops = ids_with_missing_values.intersection(dropped_ids)\n",
    "\n",
    "# 6. Display the Report\n",
    "print(f\"--- Data Integrity Report ---\")\n",
    "print(f\"IDs with missing values in raw files: {sorted(list(ids_with_missing_values))}\")\n",
    "print(f\"Total count of IDs with missing values: {len(ids_with_missing_values)}\")\n",
    "print(f\"Note: {nan_id_count} records had missing subject_ids entirely (untrackable).\")\n",
    "\n",
    "print(f\"\\nTotal IDs dropped from the 1-1000 range: {len(dropped_ids)}\")\n",
    "print(f\"Dropped IDs: {sorted(list(dropped_ids))}\")\n",
    "\n",
    "print(f\"\\n--- Verification ---\")\n",
    "if ids_with_missing_values.issubset(dropped_ids):\n",
    "    print(\"✅ Success: All IDs identified with missing values were removed from the final dataset.\")\n",
    "else:\n",
    "    remaining_problem_ids = ids_with_missing_values - dropped_ids\n",
    "    print(f\"⚠️ Warning: Some IDs with missing values are still in the final data: {remaining_problem_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ce2c0c-b7e7-46c1-9251-346b090c99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values in woodlawn.csv ---\n",
      "race_ethnicity    10\n",
      "dtype: int64\n",
      "--- Missing Values in hyde_park.csv ---\n",
      "race_ethnicity    17\n",
      "dtype: int64\n",
      "--- Missing Values in outcome_data.csv ---\n",
      "subject_id    2\n",
      "hours         2\n",
      "dtype: int64\n",
      "--- Missing Values in treatment.csv ---\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of target files\n",
    "files = ['woodlawn.csv', 'hyde_park.csv', 'outcome_data.csv', 'treatment.csv']\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    print(f\"--- Missing Values in {file} ---\")\n",
    "    \n",
    "    # Identify missing values with isnull() and calculate the sum per column\n",
    "    missing_counts = df.isnull().sum()\n",
    "    \n",
    "    # Filter and display only columns that have at least one missing value for better readability\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e1158-d4a9-471c-bd46-c55329e41b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
